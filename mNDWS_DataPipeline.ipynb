{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c242736c-a160-4437-bc4e-2be2000fcb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# 0) Setup (Colab installs) + Utilities\n",
    "# =========================================================\n",
    "!pip -q install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu121\n",
    "!pip -q install numpy pandas scikit-learn einops tqdm\n",
    "!pip -q install kagglehub tensorflow\n",
    "\n",
    "import os, math, random, glob\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
    "\n",
    "def set_seed(seed=1337):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "set_seed(1337)\n",
    "\n",
    "device   = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76f297ee-398a-48ca-ae64-68635c74d5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NPZ_ROOT -> /scratch/jbm2rt/wildfire_npz_tiles_mndws_v1\n",
      "Using existing NPZ tiles at /scratch/jbm2rt/wildfire_npz_tiles_mndws_v1 (found 20097 files)\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# 1) Data: use existing NPZ tiles; if missing, build from Kaggle TFRecords (mNDWS)\n",
    "# =========================================================\n",
    "import os, glob, math, getpass\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def pick_npz_root(subdir=\"wildfire_npz_tiles_mndws_v1\"):\n",
    "    user = os.environ.get(\"USER\") or getpass.getuser() or \"user\"\n",
    "    candidates = [\n",
    "        os.environ.get(\"NPZ_ROOT\"),                                   # explicit override (full path)\n",
    "        os.path.join(os.environ[\"SCRATCH\"], subdir) if \"SCRATCH\" in os.environ else None,\n",
    "        f\"/scratch/{user}/{subdir}\",                                   # Rivanna default\n",
    "        f\"/content/{subdir}\" if os.path.isdir(\"/content\") else None,   # Colab\n",
    "        os.path.join(os.path.expanduser(\"~\"), subdir),                 # fallback\n",
    "    ]\n",
    "    for p in candidates:\n",
    "        if not p: \n",
    "            continue\n",
    "        try:\n",
    "            os.makedirs(p, exist_ok=True)\n",
    "            return p\n",
    "        except OSError:\n",
    "            continue\n",
    "    raise RuntimeError(\"Could not create NPZ root in any candidate location\")\n",
    "\n",
    "NPZ_ROOT = pick_npz_root()\n",
    "print(\"NPZ_ROOT ->\", NPZ_ROOT)\n",
    "\n",
    "def have_npz(root):\n",
    "    return len(glob.glob(os.path.join(root, \"*.npz\"))) > 0\n",
    "\n",
    "if not have_npz(NPZ_ROOT):\n",
    "    print(\"No NPZ tiles found — converting from mNDWS TFRecords...\")\n",
    "    import kagglehub, tensorflow as tf\n",
    "\n",
    "    # Modified Next Day Wildfire Spread dataset\n",
    "    path = kagglehub.dataset_download(\"georgehulsey/modified-next-day-wildfire-spread\")\n",
    "    print(\"Kaggle dataset path:\", path)\n",
    "\n",
    "    # Look recursively in case TFRecords are split by split folders\n",
    "    tfrecs = sorted(\n",
    "        glob.glob(os.path.join(path, \"**\", \"*.tfrecord\"), recursive=True)\n",
    "        + glob.glob(os.path.join(path, \"**\", \"*.tfrecords\"), recursive=True)\n",
    "    )\n",
    "    assert len(tfrecs) > 0, \"No TFRecords found in mNDWS dataset.\"\n",
    "\n",
    "    os.makedirs(NPZ_ROOT, exist_ok=True)\n",
    "\n",
    "    # mNDWS expected feature keys (each 64x64 flattened to 4096)\n",
    "    # Include viirs_* masks + legacy aliases for robustness.\n",
    "    keys = [\n",
    "        # labels/masks\n",
    "        \"viirs_PrevFireMask\",\"viirs_FireMask\",\n",
    "        \"PrevFireMask\",\"FireMask\",\n",
    "        # vegetation/topography\n",
    "        \"NDVI\",\"elevation\",\n",
    "        # population for barrier proxy\n",
    "        \"population\",\n",
    "        # mNDWS meteorology\n",
    "        \"avg_sph\",\"tmp_day\",\"tmp_75\",\n",
    "        \"wind_avg\",\"wdir_wind\",\"gust_med\",\"wdir_gust\",\"wind_75\",\n",
    "        # hydrology/landcover\n",
    "        \"water\",\"impervious\",\n",
    "        # drought and precip\n",
    "        \"pdsi\",\"pr\",\n",
    "        # fire danger and embeddings\n",
    "        \"erc\",\"bi\",\"chili\",\"fuel1\",\"fuel2\",\"fuel3\",\n",
    "        # optional id\n",
    "        \"sample_id\"\n",
    "    ]\n",
    "\n",
    "    def read_flat_float32(feat):\n",
    "        fl = feat.float_list.value\n",
    "        if len(fl) == 0: return None\n",
    "        arr = np.asarray(fl, dtype=np.float32)\n",
    "        if arr.size == 4096:\n",
    "            return arr.reshape(64, 64)\n",
    "        s = int(round(math.sqrt(arr.size)))\n",
    "        assert s*s == arr.size, f\"Unexpected length {arr.size}\"\n",
    "        return arr.reshape(s, s)\n",
    "\n",
    "    def wind_uv_by(speed, theta):\n",
    "        if speed is None or theta is None:\n",
    "            return None, None\n",
    "        th = theta.copy()\n",
    "        # mNDWS dirs commonly in radians ([-pi, pi]); convert if degrees.\n",
    "        if np.nanmax(np.abs(th)) > 6.4:\n",
    "            th = np.deg2rad(th % 360.0)\n",
    "        u = speed * np.cos(th)\n",
    "        v = speed * np.sin(th)\n",
    "        return u.astype(np.float32), v.astype(np.float32)\n",
    "\n",
    "    def slope_aspect_from_elevation(z):\n",
    "        gy, gx = np.gradient(z.astype(np.float32))\n",
    "        mag = np.sqrt(gx**2 + gy**2)\n",
    "        q95 = np.percentile(mag, 95) + 1e-6\n",
    "        slope  = np.clip(mag / q95, 0, 1).astype(np.float32)\n",
    "        aspect = np.arctan2(-gy, -gx).astype(np.float32)  # [-pi, pi]\n",
    "        return slope, aspect\n",
    "\n",
    "    def ndvi_to_01(ndvi):\n",
    "        nd = ndvi.astype(np.float32)\n",
    "        # mNDWS NDVI is scaled (e.g., [-2000..9987]). Convert to [-1,1] via /10000\n",
    "        if np.nanmax(np.abs(nd)) > 2.0:\n",
    "            nd = nd / 10000.0\n",
    "        # Map [-1,1] -> [0,1]\n",
    "        nd = np.clip((nd + 1.0) / 2.0, 0, 1)\n",
    "        return nd.astype(np.float32)\n",
    "\n",
    "    def rh_from_avg_sph(avg_sph, tmp_day, tmp_75):\n",
    "        # Proxy RH: normalize specific humidity by its 95th percentile\n",
    "        # and damp by diurnal range signal (tmp_75 - tmp_day).\n",
    "        if avg_sph is None:\n",
    "            return np.zeros((64,64), np.float32)\n",
    "        s95 = np.percentile(avg_sph, 95) + 1e-6\n",
    "        rh = np.clip(avg_sph / s95, 0, 1)\n",
    "        if tmp_day is not None and tmp_75 is not None:\n",
    "            tr = np.clip((tmp_75 - tmp_day), 0, 20) / 20.0\n",
    "            rh = rh * (1.0 - 0.5*tr)\n",
    "        return np.clip(rh, 0, 1).astype(np.float32)\n",
    "\n",
    "    def barrier_from_population(pop):\n",
    "        if pop is None:\n",
    "            return np.zeros((64,64), np.float32)\n",
    "        pop = np.clip(pop, 0, None).astype(np.float32)\n",
    "        thr = np.percentile(pop, 90)\n",
    "        return (pop >= thr).astype(np.float32)\n",
    "\n",
    "    def pick_first(*vals):\n",
    "        for v in vals:\n",
    "            if v is not None:\n",
    "                return v\n",
    "        return None\n",
    "\n",
    "    converted = 0\n",
    "    skipped_missing_masks = 0\n",
    "\n",
    "    for f in tqdm(tfrecs, desc=\"Converting TFRecords → NPZ (mNDWS)\"):\n",
    "        for raw in tf.data.TFRecordDataset(f):\n",
    "            ex = tf.train.Example.FromString(raw.numpy()).features.feature\n",
    "            A = {k: read_flat_float32(ex[k]) if k in ex else None for k in keys}\n",
    "\n",
    "            # Masks: prefer viirs_*; fall back to legacy names\n",
    "            prev_mask = pick_first(A.get(\"viirs_PrevFireMask\"), A.get(\"PrevFireMask\"))\n",
    "            next_mask = pick_first(A.get(\"viirs_FireMask\"),     A.get(\"FireMask\"))\n",
    "            if prev_mask is None or next_mask is None:\n",
    "                skipped_missing_masks += 1\n",
    "                continue\n",
    "\n",
    "            # Labels\n",
    "            prev_fire = (prev_mask > 0.5).astype(np.float32)\n",
    "            next_fire = (next_mask > 0.5).astype(np.float32)\n",
    "\n",
    "            # Temperature: use mNDWS daily mean (tmp_day)\n",
    "            if A[\"tmp_day\"] is not None:\n",
    "                temp = A[\"tmp_day\"].astype(np.float32)\n",
    "            else:\n",
    "                temp = np.zeros((64,64), np.float32)\n",
    "\n",
    "            # Wind: use wind_avg + wdir_wind (mNDWS)\n",
    "            u, v = wind_uv_by(A[\"wind_avg\"], A[\"wdir_wind\"])\n",
    "            if u is None or v is None:\n",
    "                # Fallback to gust-based if average missing\n",
    "                u, v = wind_uv_by(A[\"gust_med\"], A[\"wdir_gust\"])\n",
    "                if u is None or v is None:\n",
    "                    u = np.zeros((64,64), np.float32)\n",
    "                    v = np.zeros((64,64), np.float32)\n",
    "\n",
    "            # NDVI: rescale from scaled ints, then to [0,1]\n",
    "            if A[\"NDVI\"] is not None:\n",
    "                ndvi = ndvi_to_01(A[\"NDVI\"])\n",
    "            else:\n",
    "                ndvi = np.full((64,64), 0.5, np.float32)\n",
    "\n",
    "            # Relative humidity proxy from avg_sph and temperatures\n",
    "            rh = rh_from_avg_sph(A[\"avg_sph\"], A[\"tmp_day\"], A[\"tmp_75\"])\n",
    "\n",
    "            # Terrain\n",
    "            if A[\"elevation\"] is not None:\n",
    "                slope, aspect = slope_aspect_from_elevation(A[\"elevation\"])\n",
    "            else:\n",
    "                slope = np.zeros((64,64), np.float32)\n",
    "                aspect = np.zeros((64,64), np.float32)\n",
    "\n",
    "            # Barrier: population-based proxy\n",
    "            barrier = barrier_from_population(A[\"population\"])\n",
    "\n",
    "            # Save minimal, consistent fields your pipeline expects.\n",
    "            fields = dict(\n",
    "                prev_fire=prev_fire, next_fire=next_fire,\n",
    "                u=u, v=v, temp=temp, rh=rh, ndvi=ndvi,\n",
    "                slope=slope, aspect=aspect, barrier=barrier\n",
    "            )\n",
    "\n",
    "            # Optional: keep extra mNDWS channels\n",
    "            def _scale01(x, denom): return np.clip((x/denom), 0, 1).astype(np.float32)\n",
    "            if A[\"impervious\"] is not None: fields[\"impervious\"] = _scale01(A[\"impervious\"], 100.0)\n",
    "            if A[\"water\"] is not None:      fields[\"water\"]      = _scale01(A[\"water\"], 100.0)\n",
    "            for k in [\"erc\",\"pdsi\",\"pr\",\"bi\",\"chili\",\"fuel1\",\"fuel2\",\"fuel3\",\"wind_75\",\"gust_med\"]:\n",
    "                if A.get(k) is not None: fields[k] = A[k].astype(np.float32)\n",
    "\n",
    "            sid_feat = ex.get(\"sample_id\", None)\n",
    "            if sid_feat and len(sid_feat.bytes_list.value) > 0:\n",
    "                sid = sid_feat.bytes_list.value[0].decode(\"utf-8\")\n",
    "            else:\n",
    "                sid = f\"{os.path.basename(f)}_{converted:07d}\"\n",
    "\n",
    "            np.savez(os.path.join(NPZ_ROOT, f\"{sid}.npz\"), **fields)\n",
    "            converted += 1\n",
    "\n",
    "    print(f\"Converted {converted} tiles → {NPZ_ROOT}\")\n",
    "    print(f\"Skipped (no masks): {skipped_missing_masks}\")\n",
    "else:\n",
    "    print(f\"Using existing NPZ tiles at {NPZ_ROOT} (found {len(glob.glob(os.path.join(NPZ_ROOT, '*.npz')))} files)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e18db338-29e9-48a7-9169-41b3baa969ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 2) Dataset & configurable channel set (mNDWS)\n",
    "# =========================================================\n",
    "import os, glob\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Base 9 channels used in NDWS workflows\n",
    "CH_ORDER_BASE = [\n",
    "    \"prev_fire\",    # 0\n",
    "    \"u\",\n",
    "    \"v\",\n",
    "    \"temp\",\n",
    "    \"rh\",\n",
    "    \"ndvi\",\n",
    "    \"slope\",\n",
    "    \"aspect\",\n",
    "    \"barrier\"       # 8\n",
    "]\n",
    "\n",
    "# Extra mNDWS channels saved by your converter (if present)\n",
    "CH_ORDER_EXTRA = [\n",
    "    \"erc\",\"pdsi\",\"pr\",\"bi\",\"chili\",\n",
    "    \"fuel1\",\"fuel2\",\"fuel3\",\n",
    "    \"impervious\",\"water\",\n",
    "    \"wind_75\",\"gust_med\"\n",
    "]\n",
    "\n",
    "# Choose what to use\n",
    "USE_CHANNELS = CH_ORDER_BASE + CH_ORDER_EXTRA  # or just CH_ORDER_BASE\n",
    "\n",
    "# Channels that should pass through unchanged (no normalization)\n",
    "DO_NOT_NORMALIZE = {\"prev_fire\", \"barrier\"}\n",
    "\n",
    "@dataclass\n",
    "class WildfirePaths:\n",
    "    root: str  # NPZ_ROOT\n",
    "\n",
    "_SPLIT_TO_ID = {\"train\": 0, \"test\": 1, \"eval\": 2, \"val\": 2}\n",
    "\n",
    "def _infer_split_from_dir(root: str, split: str):\n",
    "    sub = \"eval\" if split == \"val\" else split\n",
    "    split_dir = os.path.join(root, sub)\n",
    "    if os.path.isdir(split_dir):\n",
    "        files = sorted(glob.glob(os.path.join(split_dir, \"*.npz\")))\n",
    "        if files:\n",
    "            return files\n",
    "    return None\n",
    "\n",
    "def _filter_by_split_id(files, split: str):\n",
    "    wanted = _SPLIT_TO_ID[split]\n",
    "    out = []\n",
    "    for f in files:\n",
    "        try:\n",
    "            with np.load(f, mmap_mode=\"r\") as z:\n",
    "                if \"split_id\" in z and int(np.array(z[\"split_id\"])) == wanted:\n",
    "                    out.append(f)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return out\n",
    "\n",
    "class WildfireDataset(Dataset):\n",
    "    def __init__(self, paths: WildfirePaths, split=\"train\", max_samples=None, seed=1337, channels=None):\n",
    "        if split not in (\"train\",\"val\",\"test\",\"eval\"):\n",
    "            raise ValueError(\"split must be one of: train|val|test|eval\")\n",
    "        self.channels = list(USE_CHANNELS if channels is None else channels)\n",
    "\n",
    "        # Prefer per-split subfolders, else split_id, else 70/15/15 fallback\n",
    "        files = _infer_split_from_dir(paths.root, split)\n",
    "        if files is None:\n",
    "            all_files = sorted(glob.glob(os.path.join(paths.root, \"*.npz\")))\n",
    "            if not all_files:\n",
    "                raise ValueError(f\"No .npz files under {paths.root}\")\n",
    "            has_split_id = False\n",
    "            for probe in all_files[:10]:\n",
    "                try:\n",
    "                    with np.load(probe, mmap_mode=\"r\") as z:\n",
    "                        if \"split_id\" in z:\n",
    "                            has_split_id = True\n",
    "                            break\n",
    "                except Exception:\n",
    "                    continue\n",
    "            if has_split_id:\n",
    "                files = _filter_by_split_id(all_files, split)\n",
    "            else:\n",
    "                rng = np.random.default_rng(seed)\n",
    "                shuffled = all_files.copy()\n",
    "                rng.shuffle(shuffled)\n",
    "                n = len(shuffled)\n",
    "                n_train = int(round(0.70 * n))\n",
    "                n_val = int(round(0.15 * n))\n",
    "                if split == \"train\":\n",
    "                    sel = np.arange(0, max(1, n_train))\n",
    "                elif split in (\"val\",\"eval\"):\n",
    "                    sel = np.arange(n_train, max(n_train+1, n_train+n_val))\n",
    "                else:\n",
    "                    sel = np.arange(n_train+n_val, n) if (n_train+n_val) < n else np.arange(n-1, n)\n",
    "                files = [shuffled[i] for i in sel]\n",
    "\n",
    "        if max_samples:\n",
    "            files = files[:max_samples]\n",
    "        if len(files) == 0:\n",
    "            any_file = sorted(glob.glob(os.path.join(paths.root, \"**\", \"*.npz\"), recursive=True))\n",
    "            if not any_file:\n",
    "                raise ValueError(f\"No .npz files under {paths.root}\")\n",
    "            files = [any_file[0]]\n",
    "\n",
    "        self.paths = paths\n",
    "        self.files = files\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        arr = np.load(self.files[i])\n",
    "        # Always require minimal fields for labeling and core physics\n",
    "        required = {\"prev_fire\",\"next_fire\",\"u\",\"v\",\"temp\",\"rh\",\"ndvi\",\"slope\",\"aspect\"}\n",
    "        missing = [k for k in required if k not in arr]\n",
    "        if missing:\n",
    "            raise KeyError(f\"{os.path.basename(self.files[i])} missing keys: {missing}\")\n",
    "\n",
    "        # Build X_raw in configured order; fill missing optional channels with zeros\n",
    "        chans = []\n",
    "        for k in self.channels:\n",
    "            if k not in arr:\n",
    "                # Gracefully handle optional extras missing in some NPZs\n",
    "                chans.append(np.zeros_like(arr[\"prev_fire\"], dtype=np.float32)[None, ...])\n",
    "            else:\n",
    "                chans.append(arr[k][None, ...].astype(np.float32))\n",
    "        X_raw = np.concatenate(chans, axis=0)\n",
    "\n",
    "        y = arr[\"next_fire\"][None, ...].astype(np.float32)\n",
    "        return {\"X_raw\": torch.from_numpy(X_raw), \"y\": torch.from_numpy(y)}\n",
    "\n",
    "# Build datasets/loaders\n",
    "paths = WildfirePaths(NPZ_ROOT)\n",
    "train_ds = WildfireDataset(paths, split=\"train\", max_samples=1200)\n",
    "val_ds   = WildfireDataset(paths, split=\"eval\",  max_samples=300)\n",
    "test_ds  = WildfireDataset(paths, split=\"test\",  max_samples=300)\n",
    "\n",
    "def make_loader(ds, batch_size=16, upweight_positive=False, shuffle=False):\n",
    "    if upweight_positive:\n",
    "        weights = []\n",
    "        for f in ds.files:\n",
    "            try:\n",
    "                y = np.load(f, mmap_mode=\"r\")[\"next_fire\"]\n",
    "                weights.append(5.0 if y.sum() > 0 else 1.0)\n",
    "            except Exception:\n",
    "                weights.append(1.0)\n",
    "        sampler = WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)\n",
    "        return DataLoader(ds, batch_size=batch_size, sampler=sampler,\n",
    "                          num_workers=0, pin_memory=use_cuda, persistent_workers=False)\n",
    "    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle,\n",
    "                      num_workers=0, pin_memory=use_cuda, persistent_workers=False)\n",
    "\n",
    "train_loader = make_loader(train_ds, batch_size=16, upweight_positive=True)\n",
    "val_loader   = make_loader(val_ds,   batch_size=16)\n",
    "test_loader  = make_loader(test_ds,  batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c8942ee-0b06-4a07-96c4-4233f9875b56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([21]),\n",
       " torch.Size([21]),\n",
       " ['prev_fire',\n",
       "  'u',\n",
       "  'v',\n",
       "  'temp',\n",
       "  'rh',\n",
       "  'ndvi',\n",
       "  'slope',\n",
       "  'aspect',\n",
       "  'barrier',\n",
       "  'erc',\n",
       "  'pdsi',\n",
       "  'pr',\n",
       "  'bi',\n",
       "  'chili',\n",
       "  'fuel1',\n",
       "  'fuel2',\n",
       "  'fuel3',\n",
       "  'impervious',\n",
       "  'water',\n",
       "  'wind_75',\n",
       "  'gust_med'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =========================================================\n",
    "# 3) Channel Stats for selected channels\n",
    "# =========================================================\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def build_channel_index(ds):\n",
    "    # Map channel name -> index for DO_NOT_NORMALIZE handling\n",
    "    name_to_idx = {name: idx for idx, name in enumerate(ds.channels)}\n",
    "    return name_to_idx\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_channel_stats(ds, n_max_samples=None, batch_size=32):\n",
    "    C = len(ds.channels)\n",
    "    sums = np.zeros(C, dtype=np.float64)\n",
    "    sqs  = np.zeros(C, dtype=np.float64)\n",
    "    count = np.zeros(C, dtype=np.float64)\n",
    "\n",
    "    loader = DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    seen = 0\n",
    "    for b in loader:\n",
    "        x = b[\"X_raw\"].numpy()          # (B,C,H,W)\n",
    "        B, Cb, H, W = x.shape\n",
    "        assert Cb == C\n",
    "        x = x.reshape(B, C, -1)\n",
    "        mask = ~np.isnan(x)\n",
    "        sums += np.where(mask, x, 0.0).sum(axis=(0, 2))\n",
    "        sqs  += np.where(mask, x*x, 0.0).sum(axis=(0, 2))\n",
    "        count += mask.sum(axis=(0, 2))\n",
    "        seen += B\n",
    "        if n_max_samples and seen >= n_max_samples:\n",
    "            break\n",
    "\n",
    "    count = np.maximum(count, 1.0)\n",
    "    mean = sums / count\n",
    "    var  = np.maximum(sqs / count - mean**2, 1e-8)\n",
    "    std  = np.sqrt(var)\n",
    "\n",
    "    # Do not normalize certain channels\n",
    "    idx = build_channel_index(ds)\n",
    "    for name in DO_NOT_NORMALIZE:\n",
    "        if name in idx:\n",
    "            mean[idx[name]] = 0.0\n",
    "            std[idx[name]]  = 1.0\n",
    "\n",
    "    return torch.tensor(mean, dtype=torch.float32), torch.tensor(std, dtype=torch.float32)\n",
    "\n",
    "meanC, stdC = compute_channel_stats(train_ds, n_max_samples=2000, batch_size=32)\n",
    "meanC, stdC = meanC.to(device), stdC.to(device)\n",
    "meanC.shape, stdC.shape, train_ds.channels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
