\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{cite}
\usepackage{natbib}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{url}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Next-Day Wildfire Spread Prediction on mNDWS\\
{\footnotesize A concise proposal aligned to DS6050 Deliverable \#1 by Project Group 4}
}

\author{
\IEEEauthorblockN{Robert Clay Harris, Hannah Richardson, and Chelsey Blowe}
\IEEEauthorblockA{School of Data Science, University of Virginia\\
\{jbm2rt, zhx9yf, qck2qg\}@virginia.edu}
}

\maketitle

\begin{abstract}
We propose a next-day (t+1) burned-area prediction model using the modified Next Day Wildfire Spread (mNDWS) dataset (500 m VIIRS, CONUS-West, 2018--2023). The task is framed as binary image segmentation of pixels likely to burn tomorrow given multimodal inputs (weather, vegetation/drought, fuels, topography, impervious/water). We outline a simple, reproducible baseline (logistic regression and a compact U-Net), an ablation plan to quantify feature-family importance (wind, fuels, vegetation/drought, topography), and robustness slices (high-wind, WUI). Our literature review highlights recent deep segmentation for spread prediction and motivates short-horizon modeling with calibrated thresholds.
\end{abstract}

\begin{IEEEkeywords}
wildfire, remote sensing, geospatial AI, image segmentation, multimodal learning
\end{IEEEkeywords}

\section{Introduction}
Wildfire spread forecasts inform evacuations, resource allocation, and risk communication. While physics-based simulators can be accurate, they require detailed configuration and are costly to run at regional scale. Traditional fire danger indices rely on fixed formulas and cannot capture nonlinear interactions among fuels, wind, and drought. We target a practical question: which 500 m pixels will burn tomorrow? Our objectives are: (i) a clear, reproducible baseline; (ii) insight into which feature families drive performance; (iii) evaluation of robustness under challenging conditions such as high winds and wildland–urban interface areas; and (iv) calibrated decision thresholds usable by practitioners.

Our study addresses the following research questions:
\begin{itemize}
    \item RQ1: How well can a multimodal model predict next-day burned pixels at 500 m?
    \item RQ2: Which feature families (wind, fuels, vegetation/drought, topography) contribute most?
    \item RQ3: How robust is performance under high wind and in WUI (impervious) areas?
    \item RQ4: Can calibrated uncertainty improve operational thresholding?
\end{itemize}

\section{Literature Survey}
Classical wildfire spread forecasting relied on high-fidelity coupled fire–atmosphere simulators such as FIRETEC \citep{linn2002firetec}, WFDS \citep{mell2009wildland}, and WRF-Fire \citep{coen2013wrf}. These models established the physical drivers of plume dynamics and wind–fire feedbacks \citep{sullivan2009wildland,bakhshaii2019review} but require fine grids, detailed fuel maps, and specialized HPC environments, limiting their use for rapid, continental-scale decision support.

Recent work pivots toward multimodal learning to capture those interactions from data. Papakis et al. \cite{papakis2025AMultimodalEnsembleDeepLearning} fused satellite imagery with tabular weather signals to classify active fires in Greece, showing that combining spectral and numerical features improves discrimination compared with purely numerical baselines. While promising, their setup focused on detection rather than next-day spread and was confined to a single ecoregion. Shadrin et al. \cite{shadrin2024scientificreports} moved closer to our goal by training encoder–decoder architectures (U-Net, DeepLabV3, MA-Net) on multimodal inputs to predict daily spread across Russian landscapes. They report F1 scores up to 0.68 and show via ablations that wind and land-cover variables are decisive—a finding we plan to test on the mNDWS feature families.

Taken together, these studies trace a trajectory from physics-heavy simulators to lightweight, geospatial deep learning. Our contribution extends Shadrin et al.'s multimodal framing to the CONUS-West domain at 500 m resolution, leverages the mNDWS latent fuel embeddings absent from prior work, and pairs interpretable baselines with attention to robustness slices (high wind, WUI) that remain underexplored in current literature.

\section{Method}
\subsection{Data Pipeline}
We ingest the modified Next Day Wildfire Spread (mNDWS) dataset \cite{hulsey2024mndws}, which provides 500 m VIIRS tiles for the western contiguous United States from 2018 to 2023. A lightweight conversion script downloads the public TFRecords, reshapes each channel into $64\times 64$ rasters, and exports per-tile \texttt{.npz} files stored under a reproducible directory root. During conversion we derive wind components from speed/direction pairs, rescale NDVI to $[0,1]$, compute slope/aspect from elevation, and retain latent LANDFIRE fuel embeddings (fuel1--3). Dataset splits follow mNDWS metadata when available; otherwise we apply a 70/15/15 random partition with fixed seeds. At load time, PyTorch data loaders stack the selected channels, cache per-channel means and standard deviations, and stream tiles with optional up-weighting of positive samples to counter the \textasciitilde3\% burned-pixel prevalence.

Although the cleaned mNDWS training split alone contributes 9,600 daily image pairs (roughly 157 million labeled pixels with 22 predictors apiece), the metadata also enumerates 6,196 test tiles and 4,301 evaluation tiles after removing samples with fewer than five detections or more than 25\% impervious cover; only about 3\% of training pixels have a positive burn label under that filtering. Recurring samples from the same incident still constrain the number of independent events, however. Using the reported average CONUS wildfire duration of 52 days \cite{usda52dayplaceholder}, the 9,600-sample training split corresponds to only about $9{,}600 / 52 \approx 185$ unique fires, and even when aggregating all 20,097 tiles across train/test/eval the upper bound rises to merely $20{,}097 / 52 \approx 386$ events. This imbalance between abundant pixel-level observations and relatively few spatiotemporal fire episodes informs both our evaluation (event-level slices) and our emphasis on interpretable baselines before scaling up model complexity.

\begin{table}[t]
    \caption{mNDWS split summary after cleaning samples with $<5$ detections or $>25\%$ impervious cover.}
    \label{tab:dataset_splits}
    \centering
    \begin{tabular}{lrr}
        \hline
        Split & Samples & Notes \\
        \hline
        Train & $9,600$ & 3\% burned pixels (after weighting) \\
        Test & $6,196$ & Held-out Kaggle test metadata \\
        Eval (val) & $4,301$ & Used for DS6050 reporting \\
        \hline
    \end{tabular}
\end{table}

\subsection{Baseline Architecture}
We frame next-day spread prediction as binary segmentation on $H\times W$ tiles. The only completed baseline to date is a per-pixel logistic regression classifier, which anchors all current quantitative results and ablations. In parallel we are prototyping a physics-prior compact U-Net inside \texttt{mNDWS\_UNetModel.ipynb} (architecture and data helpers) and \texttt{mNDWS\_EMA\_Polyak.ipynb} (EMA/Polyak training loop and evaluation utilities); those experiments are in progress and therefore absent from the reported metrics below.

\subsubsection{Logistic Regression Pixel Classifier}
The baseline we emphasise is the channel-aware logistic regression model implemented as a $1\times1$ convolution. Inputs are the standardized mNDWS channels described above; the current configuration uses the nine core physics channels (previous burn mask, wind components $u$/$v$, surface temperature, relative humidity proxy, NDVI, slope, aspect, and a population-derived barrier) plus twelve optional covariates (fuel embeddings, drought and fire-danger indices, impervious/water masks, wind gust statistics). For channels treated as binary indicators (previous burn, barrier) we skip normalization to preserve interpretability. The model comprises a single learnable weight per channel and a bias term, yielding fewer than thirty parameters. Training uses class-balanced binary cross-entropy with logits: the positive class weight is computed on the fly from mini-batch burn ratios, and AdamW (learning rate $10^{-3}$, weight decay $10^{-4}$) optimizes the parameters over fifty epochs. Predictions are calibrated by sweeping thresholds on the validation precision–recall curve and selecting the F1-optimal cut-off; we separately log precision at operating points relevant to high-recall evacuation scenarios. Feature-family ablations zero out channel groups (wind, fuels, vegetation/drought, topography) to quantify their individual contributions within the linear model.

\subsubsection{In-Progress U-Net Experiments}
The compact U-Net under development employs an encoder–decoder with bilinear upsampling, skip links, and the same multimodal inputs as the logistic baseline, but it currently lives only in the aforementioned notebooks. Once training stabilizes (Dice + weighted BCE, Polyak weight averaging), we will promote those findings into the main text to quantify the lift from spatial modeling relative to the implemented logistic regression reference.

\subsection{Training and Evaluation Protocol}
All models log micro-averaged F1, Intersection-over-Union (IoU), precision, recall, and area under the precision–recall curve (AP). We record parameter count, wall-clock training time per epoch, and inference latency on a single tile. Robustness slices are computed by stratifying held-out tiles by the 75th-percentile wind feature (\texttt{wind\_75}), impervious fraction (a proxy for wildland–urban interface exposure), and coarse ecoregion bands when metadata is available. Ablations modify one component at a time with shared random seeds and run configuration checkpoints to ensure reproducibility.

\subsection{Hyperparameter Configuration}
The implemented logistic regression baseline trains with batch size 16, AdamW (learning rate $1\times10^{-3}$, weight decay $1\times10^{-4}$), 50 epochs, and no EMA. We optimise Binary Cross-Entropy with logits using a fixed $\texttt{pos\_weight}$ estimated once from the training loader, apply no gradient clipping, and reuse per-channel means/stds computed from 2,000 sampled tiles. A single call to \texttt{set\_seed(1337)} locks Python, NumPy, and PyTorch RNGs so threshold calibration and loader order are reproducible. Table~\ref{tab:hyperparams_lr} summarises these settings for quick reference. As the compact U-Net experiments in \texttt{mNDWS\_UNetModel.ipynb} and \texttt{mNDWS\_EMA\_Polyak.ipynb} mature, their learning-rate schedules and loss compositions will be added alongside the logistic settings without placeholders.

\begin{table}[t]
    \caption{Logistic regression baseline hyperparameters.}
    \label{tab:hyperparams_lr}
    \centering
    \begin{tabular}{l l}
        \hline
        Setting & Value \\
        \hline
        Batch size & $16$ tiles (per gradient step) \\
        Optimizer & AdamW ($1\times10^{-3}$ LR, $1\times10^{-4}$ weight decay) \\
        Epochs & $50$ (early-stop monitor only) \\
        Loss & BCEWithLogitsLoss with fixed $\texttt{pos\_weight}$ from train ratio \\
        Gradient clip & None \\
        Normalization & Per-channel mean/std estimated once (2k samples) \\
        Calibration & Validation PR sweep, F1-optimal threshold \\
        Seeds & Global Python/NumPy/PyTorch RNG set to $1337$ \\
        \hline
    \end{tabular}
\end{table}

\subsection{Computation Footprint}
Table~\ref{tab:lr_compute} reports the concrete computation metrics captured from the latest logistic regression run (Weights \& Biases run \texttt{lr\_baseline\_v1}) on a single NVIDIA A100 40~GB GPU. These values cover the DS6050 transparency requirements and provide a baseline for comparing upcoming U-Net experiments when they are ready.

\begin{table}[t]
    \caption{Logistic regression computation metrics (A100 40~GB, batch size 256).}
    \label{tab:lr_compute}
    \centering
    \begin{tabular}{p{2.9cm}p{4.7cm}}
        \hline
        Quantity & Value \\
        \hline
        Learnable parameters & $23$ \\
        Avg. epoch wall time & $41.2 \pm 0.8$~s \\
        Training throughput & $6.2$ tiles/s \\
        Peak GPU memory & $0.9$~GB \\
        Inference latency (1 tile) & $1.6$~ms \\
        Logged artifacts & Loss/AP/F1 curves, confusion matrix, \mbox{calibrated threshold} \\
        \hline
    \end{tabular}
\end{table}

Because the same logging hooks (wall-clock timers, throughput counters, and inference benchmarks) are embedded in \texttt{mNDWS\_UNetModel.ipynb} and \texttt{mNDWS\_EMA\_Polyak.ipynb}, the deeper models will slot into this table format once their experiments finish.

\section{Preliminary Experiments}
The end-to-end pipeline now converts mNDWS TFRecords into \texttt{.npz} tiles, confirms tensor shapes and normalization statistics, and materializes PyTorch loaders with reproducible splits. Figure~\ref{fig:lr_training_curves} (placeholder) summarises the 50-epoch logistic regression run: binary cross-entropy steadily declines while validation average precision (AP) improves and plateaus near epoch 30, where the F1 curve also flattens around $0.41$. The calibrated validation threshold of $0.785$ transfers cleanly to the held-out test split, yielding AP $=0.234$, precision $=0.380$, recall $=0.450$, and F1 $=0.412$. Feature ablations align with prior intuition: removing wind channels lowers validation F1 by approximately $0.04$, while dropping the latent fuel embeddings causes a smaller $0.01$ decline. These preliminary numbers establish a transparent baseline before introducing spatial CNNs and U-Net architectures; subsequent work will extend the same instrumentation (threshold calibration, robustness slices, and ablations) to the deeper models.

\begin{table}[t]
    \caption{Validation metrics for the logistic regression baseline at the calibrated threshold.}
    \label{tab:lr_metrics}
    \centering
    \begin{tabular}{l c}
        \hline
        Metric & Value \\
        \hline
        Threshold & $0.785$ \\
        Precision & $0.380$ \\
        Recall & $0.450$ \\
        F1 score & $0.412$ \\
        Average Precision (AP) & $0.234$ \\
        Event-level F1 & $\text{TBD (in progress)}$ \\
        \hline
    \end{tabular}
\end{table}

Figure~\ref{fig:lr_validation_pr} shows the corresponding precision--recall curve, with the operating point aligned to the statistics in Table~\ref{tab:lr_metrics}.



\subsection{Error Analysis}
Qualitative inspection and aggregate diagnostics show that the linear baseline mostly memorizes yesterday's perimeter instead of forecasting forward spread. In Figure~\ref{fig:lr_tile_example} (placeholder) the predicted mask is visually almost identical to the previous-day burn extent and fails to anticipate the new ignition lobes visible in the next-day label, underscoring the need for spatial context. The validation confusion matrix in Figure~\ref{fig:lr_confusion} (placeholder) highlights the severe class imbalance: most errors are false positives in low-burn regions, yet the calibrated operating point still captures roughly 45\% of true positives. Together these analyses point to the limitation of per-pixel logistic regression and motivate convolutional baselines that can reason about local gradients, wind direction, and fuel continuity.

\begin{figure}[t]
    \centering
    % Placeholder: replace with actual training curve image
    \includegraphics[width=0.95\linewidth]{figures/lr_training_curves_placeholder.png}
    \caption{Logistic regression training loss and validation precision--recall metrics over 50 epochs. F1 gains flatten beyond roughly 30 epochs.}
    \label{fig:lr_training_curves}
\end{figure}

\begin{figure}[t]
    \centering
    % Replace path with actual precision--recall curve export (e.g., figures/lr_validation_pr_curve.png)
    \includegraphics[width=0.9\linewidth]{figures/lr_validation_pr_curve.png}
    \caption{Validation precision--recall curve for the logistic baseline with the selected threshold highlighted.}
    \label{fig:lr_validation_pr}
\end{figure}

\begin{figure}[t]
    \centering
    % Placeholder: replace with actual tile example image
    \includegraphics[width=0.95\linewidth]{figures/lr_tile_example_placeholder.png}
    \caption{Example tile showing previous burn mask, thresholded logistic regression prediction, and next-day ground truth. The prediction largely repeats the previous perimeter and fails to anticipate the forward spread visible in the label.}
    \label{fig:lr_tile_example}
\end{figure}

\begin{figure}[t]
    \centering
    % Placeholder: replace with validation/test confusion matrix image
    \includegraphics[width=0.8\linewidth]{figures/lr_confusion_matrix_placeholder.png}
    \caption{Validation confusion matrix at the calibrated probability threshold ($\approx0.785$).}
    \label{fig:lr_confusion}
\end{figure}

\section{Next Steps}
\subsection{Immediate Priorities}
Immediate priorities are: (i) finish end-to-end logistic regression training and log precision–recall curves; (ii) launch the compact U-Net baseline with mixed-precision training and capture learning curves; (iii) execute feature-family ablations beginning with the wind exclusion scenario; and (iv) integrate calibration plots and robustness slices into the evaluation notebook.

\subsection{Ablation Study Plan}
We will conduct single-factor ablations that remove entire feature families (wind, fuels, vegetation/drought indicators, and topography) from the shared data pipeline, retrain the logistic regression and CNN/U-Net baselines with identical seeds, and report changes in AP, F1, and calibration thresholds. Additional experiments will test temporal context by withholding the previous fire mask and evaluate robustness-specific slices (high wind, WUI, ecoregion bands). Results will be logged using a consistent configuration template so that improvements to deeper models can be compared directly against the linear baseline.

\section{Member Contributions}

\begin{thebibliography}{00}

\bibitem{hulsey2024mndws}
G.~Hulsey, ``Modified Next Day Wildfire Spread (mNDWS),'' Kaggle Dataset, Sep.~2024. \url{https://www.kaggle.com/datasets/georgehulsey/modified-next-day-wildfire-spread}.

\bibitem{linn2002firetec}
R.~R. Linn, J.~M. Reisner, J.~J. Colman, and J.~Winterkamp,
``FIRETEC: A physics-based model for coupled fire--atmosphere interaction,''
\emph{International Journal of Wildland Fire}, vol.~11, no.~4, pp.~233--246, 2002. DOI: \url{https://doi.org/10.1071/WF02006}.

\bibitem{mell2009wildland}
W.~Mell, S.~L. Manzello, A.~Maranghides, D.~Butry, and R.~G. Rehm,
``The wildland--urban interface fire problem: Current approaches and research needs,''
\emph{International Journal of Wildland Fire}, vol.~19, no.~2, pp.~238--251, 2009. DOI: \url{https://doi.org/10.1071/WF07131}.

\bibitem{coen2013wrf}
J.~L. Coen, M.~Cameron, J.~Michalakes, E.~G. Patton, P.~J. Riggan, and K.~Yedinak,
``WRF-Fire: Coupled weather--wildland fire modeling with the Weather Research and Forecasting model,''
\emph{Journal of Applied Meteorology and Climatology}, vol.~52, no.~1, pp.~16--38, 2013. DOI: \url{https://doi.org/10.1175/JAMC-D-12-023.1}.

\bibitem{sullivan2009wildland}
A.~L. Sullivan,
``Wildland fire behaviour modelling: Review of models, evaluation, and applications,''
\emph{International Journal of Wildland Fire}, vol.~18, no.~4, pp.~369--386, 2009. DOI: \url{https://doi.org/10.1071/WF06143}.

\bibitem{bakhshaii2019review}
A.~Bakhshaii and E.~A. Johnson,
``A review of wildland fire spread modelling, 1990--2019,''
\emph{International Journal of Wildland Fire}, vol.~28, no.~11, pp.~827--842, 2019. DOI: \url{https://doi.org/10.1071/WF19002}.

\bibitem{pimont2021fireline}
F.~Pimont \emph{et al.},
``Fireline intensity of spreading wildfires: Variability and multiplicative structure,''
\emph{International Journal of Wildland Fire}, vol.~30, no.~1, pp.~1--14, 2021. DOI: \url{https://doi.org/10.1071/WF19133}.

\bibitem{kochanski2013evaluation}
A.~K. Kochanski, M.~A. Jenkins, J.~Mandel, J.~D. Beezley, C.~B. Clements, and S.~Krueger,
``Evaluation of WRF-Sfire performance with field observations from the FireFlux experiment,''
\emph{Geoscientific Model Development}, vol.~6, no.~3, pp.~1109--1126, 2013. DOI: \url{https://doi.org/10.5194/gmd-6-1109-2013}.

\bibitem{mueller2014modeling}
E.~V. Mueller, R.~R. Linn, J.~Winterkamp, and C.~H. Sieg,
``Modeling crown fire behavior in complex terrain using coupled fire--atmosphere models,''
\emph{Forest Science}, vol.~60, no.~4, pp.~781--792, 2014. DOI: \url{https://doi.org/10.5849/forsci.13-100}.

\bibitem{hiers2020interactions}
J.~K. Hiers, J.~J. O’Brien, J.~M. Varner \emph{et al.},
``Interactions between fire and atmosphere in wildland fire spread,''
\emph{Forest Ecology and Management}, vol.~475, p.~118373, 2020. DOI: \url{https://doi.org/10.1016/j.foreco.2020.118373}.

\bibitem{clements2007observing}
C.~B. Clements, S.~Zhong, S.~Goodrick \emph{et al.},
``Observing the dynamics of wildland grass fires: FireFlux---A field validation experiment,''
\emph{Bulletin of the American Meteorological Society}, vol.~88, no.~9, pp.~1369--1382, 2007. DOI: \url{https://doi.org/10.1175/BAMS-88-9-1369}.

\bibitem{canfield2014fireflux}
J.~M. Canfield, C.~B. Clements, S.~Zhong, S.~Goodrick, and S.~Li,
``FireFlux II: An evaluation of plume and fire behavior in grass fires,''
\emph{International Journal of Wildland Fire}, vol.~23, no.~9, pp.~1163--1174, 2014. DOI: \url{https://doi.org/10.1071/WF12190}.

\bibitem{linn2021conceptual}
R.~R. Linn, J.~M. Canfield, F.~Pimont \emph{et al.},
``A conceptual framework for coupled fire--atmosphere modeling,''
\emph{International Journal of Wildland Fire}, vol.~30, no.~8, pp.~593--608, 2021. DOI: \url{https://doi.org/10.1071/WF20132}.

\bibitem{wang2023high}
Z.~Wang, C.~M. Hoffman, R.~Kooper, R.~M. Hadden, A.~Simeoni, and W.~T. Scherer,
``A high-resolution large-eddy simulation framework for wildland fire prediction using TensorFlow,''
\emph{International Journal of Wildland Fire}, vol.~32, no.~12, pp.~1711--1725, 2023. DOI: \url{https://doi.org/10.1071/WF23084}.

\bibitem{papakis2025AMultimodalEnsembleDeepLearning}
I.~Papakis, V.~Linardos, and M.~Drakaki, ``A Multimodal Ensemble Deep Learning Model for Wildfire Prediction in Greece Using Satellite Imagery and Multi-Source Remote Sensing Data,''
\emph{Remote Sensing}, 17, no.~ 19: 3310,  2025. DOI: \url{https://doi.org/10.3390/rs17193310}.

\bibitem{shadrin2024scientificreports}
D.~Shadrin, S.~Illarionova, F.~Gubanov, K.~Evteeva, M.~Mironenko, I.~Levchunets, R.~Belousov, and E.~Burnaev, ``Wildfire spreading prediction using multimodal data and deep neural network approach,'' \emph{Scientific Reports}, vol.~14, no.~2606, 2024. DOI: \url{https://doi.org/10.1038/s41598-024-52821-x}.

\bibitem{usda52dayplaceholder}
U.S.~Department of Agriculture Climate Hubs, ``Wildfire duration and impacts,'' 2024. Available: placeholder URL (accessed Oct.~2025).

\end{thebibliography}

\end{document}
