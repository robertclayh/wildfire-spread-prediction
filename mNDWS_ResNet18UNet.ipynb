{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39bd8f1e",
   "metadata": {},
   "source": [
    "# ResNet-18 U-Net for mNDWS Next-Day Wildfire Spread\n",
    "This notebook implements a reproducible, higher-capacity segmentation baseline aligned with the Milestone-2 feedback: a compact U-Net that leverages a ResNet-18 encoder, focal(\\u03b3=2)+Dice loss, stratified loaders from `mNDWS_DataPipeline`, and metric logging compatible with the logistic-regression and EMA notebooks.\n",
    "\n",
    "**Highlights**:\n",
    "- Uses the shared NPZ tiles + channel stats so it plugs into the same pipeline as earlier deliverables.\n",
    "- Tracks epoch timing, tile throughput, GPU memory, AP/F1/threshold, and saves the best checkpoint under `~/wildfire_artifacts/resnet18_unet`.\n",
    "- Includes hooks for ablations (wind toggles, loss variants) and visual diagnostics for qualitative review.\n",
    "- Designed for a single 12-core / 32 GB RAM workstation with a 4-core GPU (e.g., RTX 4060/4070); default batch sizes keep VRAM < 10 GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4904cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# 0) Environment setup, deterministic seeds, device selection\n",
    "# =============================================================\n",
    "import os, math, random, time, json, gc, pathlib, shutil\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Dict, List, Tuple\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torchvision import models\n",
    "\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
    "\n",
    "os.environ.setdefault(\"MNDWS_PIPELINE_SILENT\", \"1\")\n",
    "import mNDWS_DataPipeline as mndws_dp\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "workspace_root = pathlib.Path.cwd()\n",
    "ART_ROOT = pathlib.Path(os.environ.get(\"ARTIFACTS_DIR\", os.path.expanduser(\"~/wildfire_artifacts\"))) / \"resnet18_unet\"\n",
    "ART_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Artifacts -> {ART_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af8a537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# 1) Data pipeline hookup + loaders + channel stats\n",
    "# =============================================================\n",
    "NPZ_ROOT = mndws_dp.NPZ_ROOT\n",
    "paths = mndws_dp.WildfirePaths(NPZ_ROOT)\n",
    "CHANNELS = list(mndws_dp.USE_CHANNELS)  # tweak here for ablations\n",
    "print(f\"Using {len(CHANNELS)} channels: {CHANNELS}\")\n",
    "\n",
    "train_ds = mndws_dp.WildfireDataset(paths, split=\"train\", max_samples=None, channels=CHANNELS, augment=False)\n",
    "val_ds   = mndws_dp.WildfireDataset(paths, split=\"eval\",  max_samples=None, channels=CHANNELS, augment=False)\n",
    "test_ds  = mndws_dp.WildfireDataset(paths, split=\"test\",  max_samples=None, channels=CHANNELS, augment=False)\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "train_loader = mndws_dp.make_loader(train_ds, batch_size=BATCH_SIZE, shuffle=True, upweight_positive=True)\n",
    "val_loader   = mndws_dp.make_loader(val_ds,   batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader  = mndws_dp.make_loader(test_ds,  batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "meanC, stdC = mndws_dp.compute_channel_stats(train_ds, n_max_samples=4000, batch_size=32)\n",
    "meanC, stdC = meanC.to(device), stdC.to(device)\n",
    "\n",
    "print(\"Channel stats computed ->\", meanC.shape, stdC.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ff0e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# 2) Visualization helpers (dataset verification requirement)\n",
    "# =============================================================\n",
    "def to_numpy_tile(sample):\n",
    "    x = sample[\"X_raw\"].numpy()\n",
    "    y = sample[\"y\"].numpy()\n",
    "    return x, y\n",
    "\n",
    "def show_samples(ds, n=5):\n",
    "    idxs = np.random.choice(len(ds), size=min(n, len(ds)), replace=False)\n",
    "    fig, axes = plt.subplots(len(idxs), 2, figsize=(8, 3*len(idxs)))\n",
    "    if len(idxs) == 1:\n",
    "        axes = np.expand_dims(axes, axis=0)\n",
    "    for row, idx in enumerate(idxs):\n",
    "        sample = ds[idx]\n",
    "        x, y = to_numpy_tile(sample)\n",
    "        burn = y[0]\n",
    "        viz = x[:3] if x.shape[0] >= 3 else np.repeat(x[:1], 3, axis=0)\n",
    "        viz = (viz - viz.min()) / (viz.max() - viz.min() + 1e-6)\n",
    "        axes[row,0].imshow(np.moveaxis(viz, 0, -1))\n",
    "        axes[row,0].set_title(f\"Tile #{idx} RGB proxy\")\n",
    "        axes[row,0].axis(\"off\")\n",
    "        axes[row,1].imshow(burn, cmap=\"hot\")\n",
    "        axes[row,1].set_title(\"Next-day burn mask\")\n",
    "        axes[row,1].axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "_ = show_samples(train_ds, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ef88f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# 3) Model definition â€“ ResNet-18 encoder + lightweight decoder\n",
    "# =============================================================\n",
    "from torchvision.models import ResNet18_Weights\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(dropout) if dropout > 0 else nn.Identity(),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_ch, skip_ch, out_ch, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.conv = ConvBlock(in_ch + skip_ch, out_ch, dropout=dropout)\n",
    "    def forward(self, x, skip):\n",
    "        x = F.interpolate(x, size=skip.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class ResNet18UNet(nn.Module):\n",
    "    def __init__(self, in_ch=15, base_ch=64, pretrained=True):\n",
    "        super().__init__()\n",
    "        weights = ResNet18_Weights.IMAGENET1K_V1 if pretrained else None\n",
    "        resnet = models.resnet18(weights=weights)\n",
    "        if in_ch != 3:\n",
    "            new_conv = nn.Conv2d(in_ch, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "            if weights is not None:\n",
    "                with torch.no_grad():\n",
    "                    new_conv.weight[:, :3] = resnet.conv1.weight\n",
    "                    if in_ch > 3:\n",
    "                        for c in range(3, in_ch):\n",
    "                            new_conv.weight[:, c:c+1] = resnet.conv1.weight[:, (c % 3):(c % 3)+1]\n",
    "            resnet.conv1 = new_conv\n",
    "        self.stem = nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu)\n",
    "        self.maxpool = resnet.maxpool\n",
    "        self.encoder1 = resnet.layer1\n",
    "        self.encoder2 = resnet.layer2\n",
    "        self.encoder3 = resnet.layer3\n",
    "        self.encoder4 = resnet.layer4\n",
    "        \n",
    "        self.center = ConvBlock(512, 512, dropout=0.1)\n",
    "        self.up3 = UpBlock(512, 256, 256)\n",
    "        self.up2 = UpBlock(256, 128, 128)\n",
    "        self.up1 = UpBlock(128, 64, 96)\n",
    "        self.up0 = UpBlock(96, 64, base_ch)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(base_ch, base_ch // 2, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(base_ch // 2, 1, kernel_size=1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        s0 = self.stem(x)                # 64\n",
    "        s1 = self.maxpool(s0)\n",
    "        e1 = self.encoder1(s1)          # 64\n",
    "        e2 = self.encoder2(e1)          # 128\n",
    "        e3 = self.encoder3(e2)          # 256\n",
    "        e4 = self.encoder4(e3)          # 512\n",
    "        bottleneck = self.center(e4)\n",
    "        d3 = self.up3(bottleneck, e3)\n",
    "        d2 = self.up2(d3, e2)\n",
    "        d1 = self.up1(d2, e1)\n",
    "        d0 = self.up0(d1, s0)\n",
    "        out = F.interpolate(d0, scale_factor=2, mode=\"bilinear\", align_corners=False)\n",
    "        return self.head(out)\n",
    "\n",
    "model = ResNet18UNet(in_ch=len(CHANNELS), base_ch=96, pretrained=True).to(device)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)/1e6:.2f} M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba446ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# 4) Losses, metrics, and normalization helpers\n",
    "# =============================================================\n",
    "def normalize_batch(x):\n",
    "    return (x - meanC.view(1, -1, 1, 1)) / (stdC.view(1, -1, 1, 1) + 1e-6)\n",
    "\n",
    "class FocalDiceLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, smooth=1e-6, dice_weight=0.5):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.smooth = smooth\n",
    "        self.dice_weight = dice_weight\n",
    "    def forward(self, logits, targets):\n",
    "        probs = torch.sigmoid(logits)\n",
    "        bce = F.binary_cross_entropy_with_logits(logits, targets, reduction=\"none\")\n",
    "        pt = torch.exp(-bce)\n",
    "        focal = ((1 - pt) ** self.gamma) * bce\n",
    "        focal = self.alpha * targets * focal + (1 - self.alpha) * (1 - targets) * focal\n",
    "        focal = focal.mean()\n",
    "        inter = torch.sum(probs * targets)\n",
    "        denom = torch.sum(probs) + torch.sum(targets)\n",
    "        dice = 1 - (2 * inter + self.smooth) / (denom + self.smooth)\n",
    "        return (1 - self.dice_weight) * focal + self.dice_weight * dice\n",
    "\n",
    "criterion = FocalDiceLoss(alpha=0.3, gamma=2.0, dice_weight=0.4)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=40, eta_min=1e-5)\n",
    "scaler = GradScaler(enabled=device.type == \"cuda\")\n",
    "\n",
    "def pr_metrics(all_logits, all_targets):\n",
    "    probs = torch.sigmoid(torch.cat(all_logits, dim=0)).flatten().cpu().numpy()\n",
    "    t = torch.cat(all_targets, dim=0).flatten().cpu().numpy()\n",
    "    if t.sum() == 0:\n",
    "        return {\"AP\": 0.0, \"best_F1\": 0.0, \"best_thr\": 0.5, \"best_prec\": 0.0, \"best_rec\": 0.0}\n",
    "    ap = average_precision_score(t, probs)\n",
    "    prec, rec, thr = precision_recall_curve(t, probs)\n",
    "    f1 = (2 * prec * rec) / (prec + rec + 1e-8)\n",
    "    idx = np.nanargmax(f1)\n",
    "    best_thr = thr[min(idx, len(thr)-1)] if len(thr) else 0.5\n",
    "    return {\"AP\": float(ap), \"best_F1\": float(np.nanmax(f1)), \"best_thr\": float(best_thr),\n",
    "            \"best_prec\": float(prec[idx]), \"best_rec\": float(rec[idx])}\n",
    "\n",
    "def random_augment(x):\n",
    "    if torch.rand(1) < 0.5:\n",
    "        x = torch.flip(x, dims=[-1])\n",
    "    if torch.rand(1) < 0.5:\n",
    "        x = torch.flip(x, dims=[-2])\n",
    "    if torch.rand(1) < 0.1:\n",
    "        x = x + 0.01 * torch.randn_like(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac905a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# 5) Training / validation loops with instrumentation\n",
    "# =============================================================\n",
    "def train_one_epoch(epoch):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    tiles = 0\n",
    "    for batch in tqdm(train_loader, desc=f\"train[{epoch:02d}]\", leave=False):\n",
    "        x = batch[\"X_raw\"].to(device, non_blocking=True)\n",
    "        y = batch[\"y\"].to(device, non_blocking=True)\n",
    "        x = random_augment(x)\n",
    "        x = normalize_batch(x)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with autocast(device_type=device.type if device.type != \"cpu\" else None, enabled=device.type != \"cpu\"):\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        losses.append(loss.item())\n",
    "        tiles += x.size(0)\n",
    "    return float(np.mean(losses)), tiles\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    logits_all, targets_all = [], []\n",
    "    for batch in tqdm(loader, desc=\"eval\", leave=False):\n",
    "        x = normalize_batch(batch[\"X_raw\"].to(device, non_blocking=True))\n",
    "        y = batch[\"y\"].to(device, non_blocking=True)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y).item()\n",
    "        losses.append(loss)\n",
    "        logits_all.append(logits.detach())\n",
    "        targets_all.append(y.detach())\n",
    "    metrics = pr_metrics(logits_all, targets_all)\n",
    "    metrics[\"loss\"] = float(np.mean(losses)) if losses else float(\"nan\")\n",
    "    return metrics\n",
    "\n",
    "def save_checkpoint(state, name):\n",
    "    path = ART_ROOT / name\n",
    "    torch.save(state, path)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5550ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# 6) Training driver (configurable epochs, patience, logging)\n",
    "# =============================================================\n",
    "MAX_EPOCHS = 40\n",
    "PATIENCE = 6\n",
    "history = {\"train_loss\": [], \"val_AP\": [], \"val_F1\": [], \"val_thr\": [], \"lr\": [], \"epoch_time\": []}\n",
    "best_ap = -1\n",
    "best_state = None\n",
    "best_epoch = -1\n",
    "tiles_seen = []\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.reset_peak_memory_stats(device)\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "    torch.cuda.empty_cache() if device.type == \"cuda\" else None\n",
    "    torch.mps.empty_cache() if hasattr(torch, \"mps\") and device.type == \"mps\" else None\n",
    "    start = time.perf_counter()\n",
    "    tr_loss, tiles = train_one_epoch(epoch)\n",
    "    metrics = evaluate(val_loader)\n",
    "    scheduler.step()\n",
    "    duration = time.perf_counter() - start\n",
    "    throughput = tiles / duration if duration > 0 else float('nan')\n",
    "    history[\"train_loss\"].append(tr_loss)\n",
    "    history[\"val_AP\"].append(metrics[\"AP\"])\n",
    "    history[\"val_F1\"].append(metrics[\"best_F1\"])\n",
    "    history[\"val_thr\"].append(metrics[\"best_thr\"])\n",
    "    history[\"lr\"].append(optimizer.param_groups[0][\"lr\"])\n",
    "    history[\"epoch_time\"].append({\"sec\": duration, \"throughput_tiles_per_s\": throughput})\n",
    "    tiles_seen.append(tiles)\n",
    "    print(f\"Epoch {epoch:02d} | train {tr_loss:.4f} | val AP {metrics['AP']:.4f} | val F1* {metrics['best_F1']:.4f} | thr {metrics['best_thr']:.3f} | {duration:.1f}s\")\n",
    "    if metrics[\"AP\"] > best_ap:\n",
    "        best_ap = metrics[\"AP\"]\n",
    "        best_state = {\"model\": model.state_dict(), \"optimizer\": optimizer.state_dict(), \"epoch\": epoch, \"metrics\": metrics}\n",
    "        best_epoch = epoch\n",
    "        save_checkpoint(best_state, \"best.pt\")\n",
    "    if epoch - best_epoch >= PATIENCE:\n",
    "        print(f\"Early stopping at epoch {epoch} (best @ {best_epoch})\")\n",
    "        break\n",
    "\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state[\"model\"])\n",
    "training_summary = {\n",
    "    \"best_epoch\": best_epoch,\n",
    "    \"best_AP\": best_ap,\n",
    "    \"history\": history,\n",
    "    \"tiles_total\": int(np.sum(tiles_seen))\n",
    "}\n",
    "if device.type == \"cuda\":\n",
    "    training_summary[\"peak_gpu_gb\"] = float(torch.cuda.max_memory_allocated(device) / (1024 ** 3))\n",
    "json.dump(training_summary, open(ART_ROOT / \"training_summary.json\", \"w\"), indent=2)\n",
    "training_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e769282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# 7) Test-set evaluation + qualitative checks\n",
    "# =============================================================\n",
    "@torch.no_grad()\n",
    "def run_test(loader, threshold=None):\n",
    "    model.eval()\n",
    "    logits_all, targets_all = [], []\n",
    "    for batch in tqdm(loader, desc=\"test\", leave=False):\n",
    "        x = normalize_batch(batch[\"X_raw\"].to(device, non_blocking=True))\n",
    "        y = batch[\"y\"].to(device, non_blocking=True)\n",
    "        logits = model(x)\n",
    "        logits_all.append(logits)\n",
    "        targets_all.append(y)\n",
    "    metrics = pr_metrics(logits_all, targets_all)\n",
    "    thr = threshold if threshold is not None else metrics[\"best_thr\"]\n",
    "    preds = torch.sigmoid(torch.cat(logits_all, dim=0))\n",
    "    binary = (preds >= thr).float()\n",
    "    confusion = {\n",
    "        \"TP\": float((binary * torch.cat(targets_all, dim=0)).sum().cpu()),\n",
    "        \"FP\": float((binary * (1 - torch.cat(targets_all, dim=0))).sum().cpu()),\n",
    "        \"FN\": float(((1 - binary) * torch.cat(targets_all, dim=0)).sum().cpu())\n",
    "    }\n",
    "    return metrics, confusion\n",
    "\n",
    "test_metrics, confusion = run_test(test_loader)\n",
    "print(\"TEST metrics:\", test_metrics)\n",
    "print(\"Confusion counts:\", confusion)\n",
    "\n",
    "def visualize_predictions(loader, n=4, threshold=None):\n",
    "    loader_iter = iter(loader)\n",
    "    thr = threshold if threshold is not None else test_metrics[\"best_thr\"]\n",
    "    fig, axes = plt.subplots(n, 3, figsize=(10, 3*n))\n",
    "    for i in range(n):\n",
    "        batch = next(loader_iter)\n",
    "        x = normalize_batch(batch[\"X_raw\"].to(device))\n",
    "        y = batch[\"y\"].to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(x)\n",
    "            prob = torch.sigmoid(logits)\n",
    "        img = batch[\"X_raw\"][:, :3].cpu().numpy()[0]\n",
    "        img = (img - img.min()) / (img.max() - img.min() + 1e-6)\n",
    "        axes[i,0].imshow(np.moveaxis(img, 0, -1))\n",
    "        axes[i,0].set_title(\"Input\")\n",
    "        axes[i,0].axis(\"off\")\n",
    "        axes[i,1].imshow(y[0,0].cpu(), cmap=\"hot\")\n",
    "        axes[i,1].set_title(\"GT\")\n",
    "        axes[i,1].axis(\"off\")\n",
    "        axes[i,2].imshow(prob[0,0].cpu(), cmap=\"viridis\")\n",
    "        axes[i,2].contour((prob[0,0].cpu() >= thr).float(), levels=[0.5], colors=\"white\")\n",
    "        axes[i,2].set_title(\"Prediction\")\n",
    "        axes[i,2].axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "visualize_predictions(test_loader, n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27db576",
   "metadata": {},
   "source": [
    "## Next steps and tuning notes\n",
    "- **Wind ablation:** toggle wind channels by editing `CHANNELS` and re-running to quantify the expected ~10% AP drop.\n",
    "- **Loss sweep:** change `FocalDiceLoss` weights to compare focal+Dice vs weighted BCE; log results into `training_summary.json`.\n",
    "- **Hyper-band:** adjust `BATCH_SIZE`, `MAX_EPOCHS`, and `CosineAnnealingLR` length; instrumentation already captures throughput and GPU memory so you can report scaling on the 12-core workstation.\n",
    "- **Logging:** drop-in integrations with Weights & Biases / MLflow can be added where the `history` dict is populated if cloud tracking becomes mandatory."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
